### Chapter 7: Logical Agents

1.  Analyze the fundamental trade-off between **model checking** (like TT-ENTAILS?) and **theorem proving** (like resolution) as methods for establishing propositional entailment ($\text{KB} \models \alpha$). Critically evaluate the factors—such as the number of models versus the potential shortness of the proof—that determine when one inference method is computationally superior to the other, particularly considering the overall worst-case complexity of propositional entailment.
2.  Propositional logic, despite demonstrating core concepts through the Wumpus World example, is limited by its inability to deal concisely with **time, space, and universal relationships**. Discuss how this inherent limitation prevents propositional logic from effectively modeling environments of unbounded size, forcing the knowledge-based agent design toward more expressive, yet computationally complex, formalisms.
3.  Explain how the architectural design of a **hybrid agent** (Figure 7.20) attempts to manage the exponential complexity inherent in logical reasoning by interleaving inference with search. Critically analyze the potential risks to the agent's completeness and correctness that arise when it must rely on a **conservative approximation** (such as a 1-CNF belief state) rather than maintaining the exact set of possible world states.
4.  Resolution is described as a **complete** inference rule for propositional logic, whereas specialized algorithms like **forward chaining** and **backward chaining** are complete only for the restricted class of **Horn clauses**. Analyze the practical engineering justification for sacrificing general completeness, as seen in Horn clause systems, in favor of specialized algorithms that offer guaranteed linear-time complexity.
5.  The **DPLL algorithm** for propositional satisfiability incorporates sophisticated backtracking search techniques (like the pure symbol and unit clause heuristics). Compare and contrast these systematic methods with local search approaches, specifically the **WALKSAT** algorithm, in terms of their respective goals (satisfiability vs. entailment) and why WALKSAT cannot reliably serve as a complete proof mechanism for certifying safety in an environment like the Wumpus World.

### Chapter 8: First-Order Logic

6.  First-Order Logic (FOL) moves beyond propositional logic by adopting an ontological commitment to **objects, relations, and quantification**. Analyze how the introduction of **terms** and **universal quantification** provides FOL with the representational conciseness necessary to describe large or infinite domains, contrasting this with the necessary exponential explosion faced by propositional logic.
7.  Critically assess the central role of **ontology**—the choice of predicates, functions, and constants—in the **knowledge engineering process**. Explain why selecting an appropriate vocabulary is not merely a stylistic choice but a fundamental step that dictates the ability of the knowledge base to concisely encode axioms and support the desired inferences.
8.  Compare the conceptual and practical differences between the **standard semantics** of First-Order Logic (with its unbounded models) and the constrained **database semantics** adopted by languages like Prolog (using the Unique-Names and Closed-World assumptions). Discuss the specific computational gains achieved by these constraints and the accompanying philosophical cost in terms of loss of expressive power and logical certainty.
9.  First-Order Logic is characterized by the need to represent universal assertions about collections of objects. Analyze how the use of **quantification over time** in **successor-state axioms** for dynamic fluents achieves significant representational conciseness in FOL compared to the enumeration required in propositional logic.
10. The text suggests that the complexity of reasoning increases with the **expressive power** of the representation language. Analyze how the introduction of the **equality symbol** (=) complicates FOL inference, discussing why specialized rules like **demodulation** are often necessary additions to standard inference procedures (like resolution) that might otherwise strictly rely on axiomatization.

### Chapter 9: Inference in First-Order Logic

11. Why is the general strategy of **propositionalization**—converting FOL sentences into ground propositional sentences via Universal Instantiation—practically infeasible for complete inference in all FOL KBs, particularly those containing **function symbols**? Analyze how the core mechanism of **unification** overcomes this limitation by enabling **lifted inference rules** (like Generalized Modus Ponens) to match patterns efficiently across variables.
12. **Generalized resolution** is presented as a complete proof procedure for all of FOL, based on **refutation completeness**. Discuss the implications of FOL entailment being **semidecidable**. Why does the soundness and refutation completeness of resolution ensure that an answer is eventually found if a sentence is entailed, while simultaneously guaranteeing that no algorithm can confirm all nonentailed sentences?
13. Compare and contrast the efficiency strategies utilized by **forward chaining** and **backward chaining** for definite clause knowledge bases. Explain how the specialized control strategy of **input resolution** can structure a resolution proof to mimic the goal-directedness and efficiency characteristic of backward chaining, particularly on Horn clause knowledge bases.
14. Analyze the computational significance of restricting an FOL knowledge base to **Datalog** (definite clauses without function symbols). Explain why this specific restriction guarantees that forward chaining terminates and is complete, whereas generalized definite clauses necessitate relying on Herbrand's theorem for completeness due to the possibility of infinite inferences.
15. Deductive databases utilize advanced optimization techniques like the **Rete algorithm** and **magic sets** to improve the efficiency of **forward chaining**. Analyze how the "magic sets" approach strategically borrows the principle of **goal-directedness** from backward chaining to drastically reduce the complexity of forward inference in large knowledge bases.
