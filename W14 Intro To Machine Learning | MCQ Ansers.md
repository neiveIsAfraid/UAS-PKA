## Solutions Section

| Q No. | Correct Answer | Supporting Quote |
| :---: | :---: | :--- |
| 1 | **C) Machine Learning** | "When the agent is a computer, we call it **machine learning**: a computer observes some data, builds a model based on the data, and uses the model as both a hypothesis about the world and a piece of software that can solve problems." |
| 2 | **C) Supervised learning** | "In **supervised learning** the agent observes input-output pairs and learns a function that maps from input to output." |
| 3 | **D) Hypothesis space** | "The function h is called a hypothesis about the world. It is drawn from a **hypothesis space** H of possible functions." |
| 4 | **B) How well it generalizes to inputs it has not yet seen (test set).** | "The true measure of a hypothesis is not how it does on the training set, but rather how well it handles inputs it has not yet seen." |
| 5 | **B) Information gain** | "The **information gain** from the attribute test on A is the expected reduction in entropy: Gain(A) = B(p / (p+n)) − Remainder(A). In fact **Gain(A)** is just what we need to implement the IMPORTANCE function." |
| 6 | **C) Overfitting** | "Overfitting is when a model fits training data too closely, capturing noise rather than generalizable patterns." |
| 7 | **D) I.i.d. (Independent and identically distributed)** | "Examples that satisfy these equations are **independent and identically distributed** or **i.i.d.**." |
| 8 | **B) Regularization** | "An alternative approach is to search for a hypothesis that directly minimizes the weighted sum of empirical loss and the complexity of the hypothesis, which we will call the total cost: Cost(h) = EmpLoss(h)+λComplexity(h)... Here λ is a hyperparameter... This technique is called **regularization**." |
| 9 | **C) Squared-error loss ($L2$)** | "Squared-error loss: L2(y, ŷ) = (y− ŷ)2"; (This loss is used for linear regression in Section 19.6.) |
| 10 | **C) Decision boundary (or Linear separator)** | "A **decision boundary** is a line (or a surface, in higher dimensions) that separates the two classes. A linear decision boundary is called a **linear separator** and data that admit such a separator are called linearly separable." |
| 11 | **C) Nonparametric model** | "**Nonparametric Model**: A model whose structure and complexity scale with the size of the training set (e.g., k-nearest neighbors)."; (Also described in Section 19.7.) |
| 12 | **B) Margin** | "We call this separator... the **maximum margin separator**. The **margin** is the width of the area bounded by dashed lines in the figure—twice the distance from the separator to the nearest example point." |
| 13 | **C) Bias or variance** | "The main reason to use ensembles is to reduce either **bias or variance** of the resulting classifier."; "Multiple simple **base models** are combined into an **ensemble model** to reduce bias or variance." |
| 14 | **C) By increasing the weight of previously misclassified examples.** | "We would like the next hypothesis to do better on the misclassified examples, so we **increase their weights** while decreasing the weights of the correctly classified examples." |
| 15 | **B) Regret** | "We measure the success of this algorithm in terms of **regret**, which is defined as the number of additional mistakes we make compared to the expert who, in hindsight, had the best prediction record." |
